# -*- coding: utf-8 -*-
"""GEN 3 SS AI

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Txp_A7f031p8Hj8if0afd4OwEJ8oJh7S
"""

!pip install SpeechRecognition
!pip install pydub
!apt install ffmpeg
!pip install googletrans==4.0.0-rc1
!pip install gtts
!pip install nltk
!pip install moviepy


import speech_recognition as sr
from pydub import AudioSegment
import nltk
from nltk.tokenize import sent_tokenize
from googletrans import Translator
from gtts import gTTS
from moviepy.editor import VideoFileClip, AudioFileClip
nltk.download('punkt')

#extract the audio from the video
# Import the necessary module
from moviepy.editor import *

# Load the video
video = VideoFileClip("/content/input.mp4")

# Extract audio
audio = video.audio

# Save the audio
audio.write_audiofile("output_audio.mp3")

#remove the audio from the video

# Import necessary libraries

# Use moviepy to remove audio from the video

clip = video
video_only = clip.without_audio()
output_video_path = "no_audio.mp4" # replace with desired output path
video_only.write_videofile(output_video_path)

#export the mp3 format to wav format
from pydub import AudioSegment

audio = AudioSegment.from_mp3("output_audio.mp3")
audio.export("output_audio.wav4", format="wav")




def generate_audio_from_english_to_tamil():


    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='en-US')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)


    translator = Translator()
    translated_text = translator.translate(final_text, src='en', dest='ta').text
    print(translated_text)

    tts= gTTS(text=translated_text,lang="ta")
    tts.save("new_audio.mp3")

    #sync the audio into the video

    def sync_audio_to_video(audio_path, empty_video_path, output_video_path):
    # Load audio and video clips
        audio_clip = AudioFileClip(audio_path)
        empty_video_clip = VideoFileClip(empty_video_path)

        # Set the audio of the empty video to the extracted audio
        synced_video_clip = empty_video_clip.set_audio(audio_clip)

        # Write the synchronized video to the output path
        synced_video_clip.write_videofile(output_video_path, codec="libx264")



    audio_path = "new_audio.mp3"
    empty_video_path = "no_audio.mp4"
    output_video_path = "synced_video.mp4"

    sync_audio_to_video(audio_path, empty_video_path, output_video_path)

def generate_audio_from_english_to_hindi():


    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='en-US')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)


    translator = Translator()
    translated_text = translator.translate(final_text, src='en', dest='hi').text
    print(translated_text)

    tts= gTTS(text=translated_text,lang="hi")
    tts.save("new_audio.mp3")

     #sync the audio into the video

    def sync_audio_to_video(audio_path, empty_video_path, output_video_path):
    # Load audio and video clips
        audio_clip = AudioFileClip(audio_path)
        empty_video_clip = VideoFileClip(empty_video_path)

        # Set the audio of the empty video to the extracted audio
        synced_video_clip = empty_video_clip.set_audio(audio_clip)

        # Write the synchronized video to the output path
        synced_video_clip.write_videofile(output_video_path, codec="libx264")


    audio_path = "new_audio.mp3"
    empty_video_path = "no_audio.mp4"
    output_video_path = "synced_video.mp4"

    sync_audio_to_video(audio_path, empty_video_path, output_video_path)

def generate_audio_from_hindi_to_tamil():


    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='hi-IN')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)


    translator = Translator()
    translated_text = translator.translate(final_text, src='hi', dest='ta').text
    print(translated_text)

    tts= gTTS(translated_text,lang="ta")
    tts.save("new_audio.mp3")



     #sync the audio into the video
    from moviepy.editor import VideoFileClip, AudioFileClip

    def sync_audio_to_video(audio_path, empty_video_path, output_video_path):
    # Load audio and video clips
        audio_clip = AudioFileClip(audio_path)
        empty_video_clip = VideoFileClip(empty_video_path)

        # Set the audio of the empty video to the extracted audio
        synced_video_clip = empty_video_clip.set_audio(audio_clip)

        # Write the synchronized video to the output path
        synced_video_clip.write_videofile(output_video_path, codec="libx264")



    audio_path = "new_audio.mp3"
    empty_video_path = "no_audio.mp4"
    output_video_path = "synced_video.mp4"

    sync_audio_to_video(audio_path, empty_video_path, output_video_path)

def generate_audio_from_hindi_to_english():


    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='hi-IN')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)


    translator = Translator()
    translated_text = translator.translate(final_text, src='hi', dest='en').text
    print(translated_text)

    tts= gTTS(text=translated_text,lang="en")
    tts.save("new_audio.mp3")



     #sync the audio into the video

    def sync_audio_to_video(audio_path, empty_video_path, output_video_path):
    # Load audio and video clips
        audio_clip = AudioFileClip(audio_path)
        empty_video_clip = VideoFileClip(empty_video_path)

        # Set the audio of the empty video to the extracted audio
        synced_video_clip = empty_video_clip.set_audio(audio_clip)

        # Write the synchronized video to the output path
        synced_video_clip.write_videofile(output_video_path, codec="libx264")



    audio_path = "new_audio.mp3"
    empty_video_path = "no_audio.mp4"
    output_video_path = "synced_video.mp4"

    sync_audio_to_video(audio_path, empty_video_path, output_video_path)

def generate_audio_from_tamil_to_english():


    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='ta-IN')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)


    translator = Translator()
    translated_text = translator.translate(final_text, src='ta', dest='en').text
    print(translated_text)

    tts= gTTS(text=translated_text,lang="en")
    tts.save("new_audio.mp3")



     #sync the audio into the video

    def sync_audio_to_video(audio_path, empty_video_path, output_video_path):
    # Load audio and video clips
        audio_clip = AudioFileClip(audio_path)
        empty_video_clip = VideoFileClip(empty_video_path)

        # Set the audio of the empty video to the extracted audio
        synced_video_clip = empty_video_clip.set_audio(audio_clip)

        # Write the synchronized video to the output path
        synced_video_clip.write_videofile(output_video_path, codec="libx264")



    audio_path = "new_audio.mp3"
    empty_video_path = "no_audio.mp4"
    output_video_path = "synced_video.mp4"

    sync_audio_to_video(audio_path, empty_video_path, output_video_path)


def generate_audio_from_tamil_to_hindi():


    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='ta-IN')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)


    translator = Translator()
    translated_text = translator.translate(final_text, src='ta', dest='hi').text
    print(translated_text)

    tts= gTTS(text=translated_text,lang="en")
    tts.save("new_audio.mp3")



     #sync the audio into the video

    def sync_audio_to_video(audio_path, empty_video_path, output_video_path):
    # Load audio and video clips
        audio_clip = AudioFileClip(audio_path)
        empty_video_clip = VideoFileClip(empty_video_path)

        # Set the audio of the empty video to the extracted audio
        synced_video_clip = empty_video_clip.set_audio(audio_clip)

        # Write the synchronized video to the output path
        synced_video_clip.write_videofile(output_video_path, codec="libx264")



    audio_path = "new_audio.mp3"
    empty_video_path = "no_audio.mp4"
    output_video_path = "synced_video.mp4"

    sync_audio_to_video(audio_path, empty_video_path, output_video_path)



def generate_english_text():

    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='en-US')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)

def generate_hindi_text():


    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='hi-IN')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)


def generate_tamil_text():


    # Initialize the recognizer
    recognizer = sr.Recognizer()

    # Load the audio and split it into 30-second chunks
    audio = AudioSegment.from_wav("output_audio.wav4")
    chunk_length = 30 * 1000  # 30 seconds in milliseconds
    chunks = [audio[i:i + chunk_length] for i in range(0, len(audio), chunk_length)]

    # Define function to process audio chunk
    def process_audio_chunk(chunk):
       # Save the chunk to a temporary file
       chunk_filename = "temp_chunk.wav"
       chunk.export(chunk_filename, format="wav")

       # Convert audio chunk to text
       with sr.AudioFile(chunk_filename) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data, language='ta-IN')

                # Use Sentence Boundary Detection
                sentences = sent_tokenize(text)

                # Combine sentences with commas
                return ", ".join(sentences)
            except:
                return ""

    # Process each chunk
    texts = [process_audio_chunk(chunk) for chunk in chunks]

    # Combine all texts
    final_text = ", ".join(texts)
    print(final_text)




def adoptive_learning():
  command=input("enter the command:")
  if command=="generate the audio of the video":
     generate_audio()
  if command=="generate the text of the video":
     def generate_text():
       audio_language=input("audio_language:")
       if audio_language=="english":
         generate_english_text()
       if audio_language=="tamil":
         generate_tamil_text()
       if audio_language=="hindi":
         generate_tamil_text()

     generate_text()


original_language = input("original_language:")
translated_language = input("translated_language")
if original_language=='tamil'and translated_language=="english":
  generate_audio_from_tamil_to_english()
  adoptivelearning=input("yes or no")
  if adoptivelearning=="yes":
    adoptive_learning()
if original_language=='tamil'and translated_language=='hindi':
  generate_audio_from_tamil_to_hindi()
  adoptivelearning=input("yes or no")
  if adoptivelearning=="yes":
    adoptive_learning()
if original_language=='english'and translated_language=='tamil':
  generate_audio_from_english_to_tamil()
  adoptivelearning=input("yes or no")
  if adoptivelearning=="yes":
    adoptive_learning()
if original_language=='english'and translated_language=='hindi':
  generate_audio_from_english_to_hindi()
  adoptivelearning=input("yes or no")
  if adoptivelearning=="yes":
    adoptive_learning()
if original_language=='hindi'and translated_language=='tamil':
  generate_audio_from_hindi_to_tamil()
  adoptivelearning=input("yes or no")
  if adoptivelearning=="yes":
    adoptive_learning()
if original_language=='hindi'and translated_language=='english':
  generate_audio_from_hindi_to_english()
  adoptivelearning=input("yes or no")
  if adoptivelearning=="yes":
    adoptive_learning()
if original_language==("tamil" or "english" or "hindi") and translated_language=="no":
    adoptive_learning()